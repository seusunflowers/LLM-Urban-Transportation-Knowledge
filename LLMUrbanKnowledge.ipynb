{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e79ddd-f794-4587-80ad-b645211d5f0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T08:53:06.963703Z",
     "iopub.status.busy": "2025-07-15T08:53:06.963703Z",
     "iopub.status.idle": "2025-07-15T08:53:10.612996Z",
     "shell.execute_reply": "2025-07-15T08:53:10.611994Z",
     "shell.execute_reply.started": "2025-07-15T08:53:06.963703Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import Node2Vec\n",
    "# from torch_geometric_temporal.dataset import METRLADatasetLoader\n",
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# import ollama\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0defa5d-d9cc-4398-96d0-e2d8a6f13886",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T08:53:10.630996Z",
     "iopub.status.busy": "2025-07-15T08:53:10.630996Z",
     "iopub.status.idle": "2025-07-15T08:53:10.643995Z",
     "shell.execute_reply": "2025-07-15T08:53:10.642996Z",
     "shell.execute_reply.started": "2025-07-15T08:53:10.630996Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_llm_inputs(tokenizer, llm_name:str, input_text:str):\n",
    "    match llm_name:\n",
    "        case 'llama31':\n",
    "            input_text = '<|start_header_id|>user<|end_header_id|>\\n' + input_text + ' <|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>'\n",
    "            # Encode input text\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        case 'qwen3':\n",
    "            messages = [{\"role\": \"user\", \"content\": input_text}]\n",
    "            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True,\n",
    "                enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    "            )\n",
    "            inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "        case _:\n",
    "            raise ValueError('Unknown LLM')\n",
    "    return inputs\n",
    "    \n",
    "\n",
    "def decode_llm_outputs(tokenizer, llm_name:str, output_sequences:list, input_len:int=0):\n",
    "    output_ids = output_sequences[0][input_len:].tolist()\n",
    "    match llm_name:\n",
    "        case 'llama31':\n",
    "            output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "        case 'qwen3':\n",
    "            try:\n",
    "                # rindex finding 151668 (</think>)\n",
    "                index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "            except ValueError:\n",
    "                index = 0\n",
    "            thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "            content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "            output_text = (thinking_content, content)\n",
    "        case _:\n",
    "            raise ValueError('Unknown LLM')\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ede529-7a10-45aa-a5bf-ebb78bace010",
   "metadata": {},
   "source": [
    "# BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f41d7395-1797-4d87-aa1e-4f80ff3c33ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T10:29:09.563902Z",
     "iopub.status.busy": "2025-07-11T10:29:09.562902Z",
     "iopub.status.idle": "2025-07-11T10:29:09.575903Z",
     "shell.execute_reply": "2025-07-11T10:29:09.575903Z",
     "shell.execute_reply.started": "2025-07-11T10:29:09.563902Z"
    }
   },
   "outputs": [],
   "source": [
    "RAW_DATASET_DIR = 'bart'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0fc3d9d-a9af-496f-8e57-9531cbb07ec8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T19:42:26.986236Z",
     "iopub.status.busy": "2025-04-22T19:42:26.985637Z",
     "iopub.status.idle": "2025-04-22T20:08:04.009763Z",
     "shell.execute_reply": "2025-04-22T20:08:04.007935Z",
     "shell.execute_reply.started": "2025-04-22T19:42:26.986187Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "LLM_NAME = 'llama31-8b'\n",
    "LLM_NAME_ = '_'.join(LLM_NAME.split('-'))\n",
    "\n",
    "# Try loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Llama3.1-8B-Instruct\", model_max_length=512)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Llama3.1-8B-Instruct-hg\", device_map=\"auto\")\n",
    "# # Convert the model to half precision (FP16) if using a supported device\n",
    "# model.half()  # This converts the model to FP16\n",
    "\n",
    "station_description_llm, station_hidden_states_llm = [], []\n",
    "time_description_llm, time_hidden_states_llm = [], []\n",
    "day_description_llm, day_hidden_states_llm = [], []\n",
    "## 提取站点信息\n",
    "station_info = pd.read_excel(f'{RAW_DATASET_DIR}/station-names.xls', index_col=0)\n",
    "for station_code, station_name in zip(station_info.index, station_info['Station Name']):\n",
    "    # Example text to generate from\n",
    "    input_text = f'Please provide details about the location, train operations, ridership demand, and landmarks nearby about the {station_name} Station ' +\\\n",
    "                 'of the San Francisco Bay Area Rapit Transit System'\n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    \n",
    "    # Disable gradient calculation (this is optional but recommended for inference)\n",
    "    with torch.no_grad():\n",
    "        # Generate output from the model\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=2048, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'])\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [33, 4096]; hidden_states[-1] 表示最后一个 token，33 表示 llama3.1-8b 的 33 层隐状态，4096 表示每个隐向量的维数\n",
    "    station_description_llm.append([station_code, station_name, generated_text])\n",
    "    station_hidden_states_llm.append(hidden_states.cpu().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "station_description_llm = DataFrame(station_description_llm, columns=['Station Code', 'Station Name', 'LLM Description'])\n",
    "station_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/station_description_{LLM_NAME_}csv.zip', compression='zip')\n",
    "station_hidden_states_llm = np.array(station_hidden_states_llm)\n",
    "## 提取时间信息\n",
    "for time in range(24):\n",
    "    # Example text to generate from\n",
    "    input_text = f'San Francisco Bay Area Rapit Transit System during {time:02d}:00-{time+1:02d}:00, service and ridership '\n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    \n",
    "    # Disable gradient calculation (this is optional but recommended for inference)\n",
    "    with torch.no_grad():\n",
    "        # Generate output from the model\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=2048, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'])\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [33, 4096]; hidden_states[-1] 表示最后一个 token，33 表示 llama3.1-8b 的 33 层隐状态，4096 表示每个隐向量的维数\n",
    "    time_description_llm.append([time, generated_text])\n",
    "    time_hidden_states_llm.append(hidden_states.cpu().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "time_description_llm = DataFrame(time_description_llm, columns=['Hour', 'LLM Description'])\n",
    "time_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/time_description_{LLM_NAME_}csv.zip', compression='zip')\n",
    "time_hidden_states_llm = np.array(time_hidden_states_llm)\n",
    "## 提取日周期信息\n",
    "for day in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']:\n",
    "    # Example text to generate from\n",
    "    input_text = f'San Francisco Bay Area Rapit Transit System on a typical {day}, service and ridership '\n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    \n",
    "    # Disable gradient calculation (this is optional but recommended for inference)\n",
    "    with torch.no_grad():\n",
    "        # Generate output from the model\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=2048, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'])\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [33, 4096]; hidden_states[-1] 表示最后一个 token，33 表示 llama3.1-8b 的 33 层隐状态，4096 表示每个隐向量的维数\n",
    "    day_description_llm.append([day, generated_text])\n",
    "    day_hidden_states_llm.append(hidden_states.cpu().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "day_description_llm = DataFrame(day_description_llm, columns=['Day', 'LLM Description'])\n",
    "day_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/day_description_{LLM_NAME_}csv.zip', compression='zip')\n",
    "day_hidden_states_llm = np.array(day_hidden_states_llm)\n",
    "    \n",
    "np.savez_compressed(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/hidden_states_{LLM_NAME_}.npz', station=station_hidden_states_llm, time=time_hidden_states_llm, day=day_hidden_states_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b63e64-fd26-4818-89f1-b2d72ffd8b1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T06:49:19.910887Z",
     "iopub.status.busy": "2025-06-11T06:49:19.909932Z",
     "iopub.status.idle": "2025-06-11T09:42:08.859828Z",
     "shell.execute_reply": "2025-06-11T09:42:08.858760Z",
     "shell.execute_reply.started": "2025-06-11T06:49:19.910835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-11 14:49:40,923] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xcc/miniconda3/envs/peng_c/compiler_compat/ld: cannot find -laio: 没有那个文件或目录\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/xcc/miniconda3/envs/peng_c/compiler_compat/ld: cannot find -lcufile: 没有那个文件或目录\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75d949396024899811cfd88d41824cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7a495df39a4ef084c1d28659640ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "LLM_NAME = 'qwen3-8b'\n",
    "LLM_NAME_ = '_'.join(LLM_NAME.split('-'))\n",
    "\n",
    "# Try loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", model_max_length=1024)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-8B\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "station_description_llm, station_hidden_states_llm = [], []\n",
    "time_description_llm, time_hidden_states_llm = [], []\n",
    "day_description_llm, day_hidden_states_llm = [], []\n",
    "\n",
    "## 提取站点信息\n",
    "station_info = pd.read_excel(f'{RAW_DATASET_DIR}/station-names.xls', index_col=0)\n",
    "for station_code, station_name in tqdm.notebook.tqdm(zip(station_info.index, station_info['Station Name'])):\n",
    "    # Example text to generate from\n",
    "    input_text = f'Please provide details about the location, train operations, ridership demand, and landmarks nearby about the {station_name} Station ' +\\\n",
    "                 'of the San Francisco Bay Area Rapit Transit System.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    station_description_llm.append([station_code, station_name, thinking_content, content])\n",
    "    station_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "station_description_llm = DataFrame(station_description_llm, columns=['Station Code', 'Station Name', 'LLM Thinking', 'LLM Description'])\n",
    "station_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/station_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "station_hidden_states_llm = np.array(station_hidden_states_llm)\n",
    "## 提取时间信息\n",
    "for time in range(24):\n",
    "    # Example text to generate from\n",
    "    input_text = f'San Francisco Bay Area Rapit Transit System during {time:02d}:00-{time+1:02d}:00, service and ridership '\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "\n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    time_description_llm.append([time, thinking_content, content])\n",
    "    time_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "time_description_llm = DataFrame(time_description_llm, columns=['Hour', 'LLM Thinking', 'LLM Description'])\n",
    "time_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/time_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "time_hidden_states_llm = np.array(time_hidden_states_llm)\n",
    "## 提取日周期信息\n",
    "for day in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']:\n",
    "    # Example text to generate from\n",
    "    input_text = f'San Francisco Bay Area Rapit Transit System on a typical {day}, service and ridership.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    day_description_llm.append([day, thinking_content, content])\n",
    "    day_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "day_description_llm = DataFrame(day_description_llm, columns=['Day', 'LLM Thinking', 'LLM Description'])\n",
    "day_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/day_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "day_hidden_states_llm = np.array(day_hidden_states_llm)\n",
    "    \n",
    "np.savez_compressed(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/hidden_states_{LLM_NAME_}.npz', station=station_hidden_states_llm, time=time_hidden_states_llm, day=day_hidden_states_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d38a662-479b-4c97-bb68-67b28739e0f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T06:29:10.248194Z",
     "iopub.status.busy": "2025-06-10T06:29:10.247546Z",
     "iopub.status.idle": "2025-06-10T09:03:20.930768Z",
     "shell.execute_reply": "2025-06-10T09:03:20.930110Z",
     "shell.execute_reply.started": "2025-06-10T06:29:10.248133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e8caf65d2e4375981ee490d5a03e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "LLM_NAME = 'qwen3-14b'\n",
    "LLM_NAME_ = '_'.join(LLM_NAME.split('-'))\n",
    "\n",
    "# Try loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-14B\", model_max_length=1024)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-14B\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "station_description_llm, station_hidden_states_llm = [], []\n",
    "time_description_llm, time_hidden_states_llm = [], []\n",
    "day_description_llm, day_hidden_states_llm = [], []\n",
    "\n",
    "## 提取站点信息\n",
    "station_info = pd.read_excel(f'{RAW_DATASET_DIR}/station-names.xls', index_col=0)\n",
    "for station_code, station_name in tqdm.notebook.tqdm(zip(station_info.index, station_info['Station Name'])):\n",
    "    # Example text to generate from\n",
    "    input_text = f'Please provide details about the location, train operations, ridership demand, and landmarks nearby about the {station_name} Station ' +\\\n",
    "                 'of the San Francisco Bay Area Rapit Transit System.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    station_description_llm.append([station_code, station_name, thinking_content, content])\n",
    "    station_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "station_description_llm = DataFrame(station_description_llm, columns=['Station Code', 'Station Name', 'LLM Thinking', 'LLM Description'])\n",
    "station_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/station_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "station_hidden_states_llm = np.array(station_hidden_states_llm)\n",
    "## 提取时间信息\n",
    "for time in range(24):\n",
    "    # Example text to generate from\n",
    "    input_text = f'San Francisco Bay Area Rapit Transit System during {time:02d}:00-{time+1:02d}:00, service and ridership '\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "\n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    time_description_llm.append([time, thinking_content, content])\n",
    "    time_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "time_description_llm = DataFrame(time_description_llm, columns=['Hour', 'LLM Thinking', 'LLM Description'])\n",
    "time_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/time_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "time_hidden_states_llm = np.array(time_hidden_states_llm)\n",
    "## 提取日周期信息\n",
    "for day in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']:\n",
    "    # Example text to generate from\n",
    "    input_text = f'San Francisco Bay Area Rapit Transit System on a typical {day}, service and ridership.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    day_description_llm.append([day, thinking_content, content])\n",
    "    day_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "day_description_llm = DataFrame(day_description_llm, columns=['Day', 'LLM Thinking', 'LLM Description'])\n",
    "day_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/day_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "day_hidden_states_llm = np.array(day_hidden_states_llm)\n",
    "    \n",
    "np.savez_compressed(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/hidden_states_{LLM_NAME_}.npz', station=station_hidden_states_llm, time=time_hidden_states_llm, day=day_hidden_states_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18433d78-0eb9-4b4c-bfe7-ba65c39e97bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T09:25:56.631322Z",
     "iopub.status.busy": "2025-06-10T09:25:56.630078Z",
     "iopub.status.idle": "2025-06-10T10:00:33.559171Z",
     "shell.execute_reply": "2025-06-10T10:00:33.557975Z",
     "shell.execute_reply.started": "2025-06-10T09:25:56.631270Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-10 17:25:58,389] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xcc/miniconda3/envs/peng_c/compiler_compat/ld: cannot find -laio: 没有那个文件或目录\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/xcc/miniconda3/envs/peng_c/compiler_compat/ld: cannot find -lcufile: 没有那个文件或目录\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a514327805e419c939d7341d8aa1398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994dc019bd004da29a692c70cefe58e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6252ad79f224de786acad7cdb6ec512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b7f57336d842ba84ff6a5fb4ebe5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "PRIME_LLM_NAME = 'qwen3-14b'\n",
    "LLM_NAME = 'llama31-8b'\n",
    "PRIME_LLM_NAME_ = '_'.join(PRIME_LLM_NAME.split('-'))\n",
    "LLM_NAME_ = PRIME_LLM_NAME_ + '_' + '_'.join(LLM_NAME.split('-'))\n",
    "\n",
    "# Try loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Llama3.1-8B-Instruct\", model_max_length=2048)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Llama3.1-8B-Instruct-hg\", device_map=\"auto\")\n",
    "# # Convert the model to half precision (FP16) if using a supported device\n",
    "# model.half()  # This converts the model to FP16\n",
    "\n",
    "station_description_llm, station_hidden_states_llm = [], []\n",
    "time_description_llm, time_hidden_states_llm = [], []\n",
    "day_description_llm, day_hidden_states_llm = [], []\n",
    "\n",
    "station_info = pd.read_csv(f'{RAW_DATASET_DIR}/description-{PRIME_LLM_NAME}/station_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "time_info = pd.read_csv(f'{RAW_DATASET_DIR}/description-{PRIME_LLM_NAME}/time_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "day_info = pd.read_csv(f'{RAW_DATASET_DIR}/description-{PRIME_LLM_NAME}/day_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "## 提取站点信息\n",
    "for station_id, prime_llm_output in tqdm.notebook.tqdm(zip(station_info['Station Name'], station_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=4096, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "    \n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    station_description_llm.append([station_id, generated_text])\n",
    "    station_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "station_description_llm = DataFrame(station_description_llm, columns=['Station Name', 'LLM Description'])\n",
    "station_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/station_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "station_hidden_states_llm = np.array(station_hidden_states_llm)\n",
    "## 提取时间信息\n",
    "for time, prime_llm_output in tqdm.notebook.tqdm(zip(time_info['Hour'], time_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=4096, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "\n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    time_description_llm.append([time, generated_text])\n",
    "    time_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "time_description_llm = DataFrame(time_description_llm, columns=['Hour', 'LLM Description'])\n",
    "time_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/time_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "time_hidden_states_llm = np.array(time_hidden_states_llm)\n",
    "## 提取日周期信息\n",
    "for day, prime_llm_output in tqdm.notebook.tqdm(zip(day_info['Day'], day_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=4096, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "    \n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    day_description_llm.append([day, generated_text])\n",
    "    day_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "day_description_llm = DataFrame(day_description_llm, columns=['Day', 'LLM Description'])\n",
    "day_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/day_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "day_hidden_states_llm = np.array(day_hidden_states_llm)\n",
    "    \n",
    "np.savez_compressed(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/hidden_states_{LLM_NAME_}.npz',\n",
    "                    station=station_hidden_states_llm, time=time_hidden_states_llm, day=day_hidden_states_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e6d23-7245-4e50-9c1a-73a2806d2146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "PRIME_LLM_NAME = 'qwen3-14b'\n",
    "LLM_NAME = 'qwen3-8b'\n",
    "PRIME_LLM_NAME_ = '_'.join(PRIME_LLM_NAME.split('-'))\n",
    "LLM_NAME_ = PRIME_LLM_NAME_ + '_' + '_'.join(LLM_NAME.split('-'))\n",
    "\n",
    "# Try loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", model_max_length=2048)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-8B\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "station_description_llm, station_hidden_states_llm = [], []\n",
    "time_description_llm, time_hidden_states_llm = [], []\n",
    "day_description_llm, day_hidden_states_llm = [], []\n",
    "\n",
    "station_info = pd.read_csv(f'{RAW_DATASET_DIR}/description-{PRIME_LLM_NAME}/station_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "time_info = pd.read_csv(f'{RAW_DATASET_DIR}/description-{PRIME_LLM_NAME}/time_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "day_info = pd.read_csv(f'{RAW_DATASET_DIR}/description-{PRIME_LLM_NAME}/day_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "## 提取站点信息\n",
    "for station_id, prime_llm_output in tqdm.notebook.tqdm(zip(station_info['Station Name'], station_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=16384, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    station_description_llm.append([station_id, thinking_content, content])\n",
    "    station_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "station_description_llm = DataFrame(station_description_llm, columns=['Station Name', 'LLM Thinking', 'LLM Description'])\n",
    "station_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/station_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "station_hidden_states_llm = np.array(station_hidden_states_llm)\n",
    "## 提取时间信息\n",
    "for time, prime_llm_output in tqdm.notebook.tqdm(zip(time_info['Hour'], time_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "\n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    time_description_llm.append([time, thinking_content, content])\n",
    "    time_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "time_description_llm = DataFrame(time_description_llm, columns=['Hour', 'LLM Thinking', 'LLM Description'])\n",
    "time_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/time_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "time_hidden_states_llm = np.array(time_hidden_states_llm)\n",
    "## 提取日周期信息\n",
    "for day, prime_llm_output in tqdm.notebook.tqdm(zip(day_info['Day'], day_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    day_description_llm.append([day, thinking_content, content])\n",
    "    day_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "day_description_llm = DataFrame(day_description_llm, columns=['Day', 'LLM Thinking', 'LLM Description'])\n",
    "day_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/day_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "day_hidden_states_llm = np.array(day_hidden_states_llm)\n",
    "    \n",
    "np.savez_compressed(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/hidden_states_{LLM_NAME_}.npz',\n",
    "                    station=station_hidden_states_llm, time=time_hidden_states_llm, day=day_hidden_states_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54407e6f-ae43-455d-bb1b-00d4d5d46151",
   "metadata": {},
   "source": [
    "# UrbanEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc95341f-11ef-444b-834e-182c9a3eaf87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:16:42.780503Z",
     "iopub.status.busy": "2025-07-11T07:16:42.780503Z",
     "iopub.status.idle": "2025-07-11T07:16:42.798502Z",
     "shell.execute_reply": "2025-07-11T07:16:42.798502Z",
     "shell.execute_reply.started": "2025-07-11T07:16:42.780503Z"
    }
   },
   "outputs": [],
   "source": [
    "RAW_DATASET_DIR = 'UrbanEV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b24620e6-5025-4cdb-846b-f133095f6c46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T08:43:31.587345Z",
     "iopub.status.busy": "2025-06-06T08:43:31.586700Z",
     "iopub.status.idle": "2025-06-07T18:08:33.867746Z",
     "shell.execute_reply": "2025-06-07T18:08:33.867103Z",
     "shell.execute_reply.started": "2025-06-06T08:43:31.587294Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-06 16:43:33,335] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xcc/miniconda3/envs/peng_c/compiler_compat/ld: cannot find -laio: 没有那个文件或目录\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/xcc/miniconda3/envs/peng_c/compiler_compat/ld: cannot find -lcufile: 没有那个文件或目录\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f391a43e1743e99e48c638e485a1d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb23b5b091042d68db1816d094a8a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "LLM_NAME = 'llama31-8b'\n",
    "LLM_NAME_ = '_'.join(LLM_NAME.split('-'))\n",
    "\n",
    "# Try loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Llama3.1-8B-Instruct\", model_max_length=4096)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Llama3.1-8B-Instruct-hg\", device_map=\"auto\")\n",
    "# # Convert the model to half precision (FP16) if using a supported device\n",
    "# model.half()  # This converts the model to FP16\n",
    "\n",
    "zone_description_llm, zone_hidden_states_llm = [], []\n",
    "time_description_llm, time_hidden_states_llm = [], []\n",
    "day_description_llm, day_hidden_states_llm = [], []\n",
    "\n",
    "zones_points = pd.read_csv(f'{RAW_DATASET_DIR}/zone-stations.csv', usecols=[2,12,14])\n",
    "## 提取站点信息\n",
    "for zone_id, sub_points in tqdm.notebook.tqdm(zones_points.groupby('TAZID')):\n",
    "    district_id_text = sub_points['DISTRICT'].iloc[0]\n",
    "    point_id_text = '、'.join(sub_points['name'])\n",
    "    input_text = f'中国广东省深圳市{district_id_text + point_id_text}附近区域的土地利用、居民组成、交通出行模式、新能源汽车友好度、新能源汽车充电需求与服务水平。'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=9216, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "    \n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'])\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    zone_description_llm.append([zone_id, generated_text])\n",
    "    zone_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "zone_description_llm = DataFrame(zone_description_llm, columns=['Zone Name', 'LLM Description'])\n",
    "zone_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/zone_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "zone_hidden_states_llm = np.array(zone_hidden_states_llm)\n",
    "## 提取时间信息\n",
    "for time in range(24):\n",
    "    # Example text to generate from\n",
    "    input_text = f'中国广东省深圳市{time:02d}:00-{time+1:02d}:00间的新能源汽车充电需求与服务水平。'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=6152, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "\n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'])\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    time_description_llm.append([time, generated_text])\n",
    "    time_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "time_description_llm = DataFrame(time_description_llm, columns=['Hour', 'LLM Description'])\n",
    "time_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/time_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "time_hidden_states_llm = np.array(time_hidden_states_llm)\n",
    "## 提取日周期信息\n",
    "for day in ['周一', '周二', '周三', '周四', '周五', '周六', '周日', '节假日']:\n",
    "    # Example text to generate from\n",
    "    input_text = f'中国广东省深圳市一个典型{day}的新能源汽车充电需求与服务水平。'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=6152, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "    \n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'])\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    day_description_llm.append([day, generated_text])\n",
    "    day_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "day_description_llm = DataFrame(day_description_llm, columns=['Day', 'LLM Description'])\n",
    "day_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/day_description_{LLM_NAME_}csv.zip', compression='zip')\n",
    "day_hidden_states_llm = np.array(day_hidden_states_llm)\n",
    "    \n",
    "np.savez_compressed(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/hidden_states_{LLM_NAME_}.npz',\n",
    "                    zone=zone_hidden_states_llm, time=time_hidden_states_llm, day=day_hidden_states_llm)\n",
    "\n",
    "zone_description_llm = pd.read_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/zone_description_{LLM_NAME_}.csv.zip', usecols=[1,2])\n",
    "zone_description_llm.to_excel(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/zone_description_{LLM_NAME_}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b57b93c8-0a9f-4171-adf1-ca15a6195bda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T20:06:13.815704Z",
     "iopub.status.busy": "2025-06-10T20:06:13.815124Z",
     "iopub.status.idle": "2025-06-11T06:13:39.928419Z",
     "shell.execute_reply": "2025-06-11T06:13:39.926866Z",
     "shell.execute_reply.started": "2025-06-10T20:06:13.815654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-11 04:06:34,696] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xcc/miniconda3/envs/peng_c/compiler_compat/ld: cannot find -laio: 没有那个文件或目录\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/xcc/miniconda3/envs/peng_c/compiler_compat/ld: cannot find -lcufile: 没有那个文件或目录\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c75679dd88414086de2c486ebcf07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65d375034404bc7afbc311b7233e906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "LLM_NAME = 'qwen3-8b'\n",
    "LLM_NAME_ = '_'.join(LLM_NAME.split('-'))\n",
    "\n",
    "# Try loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", model_max_length=4096)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-8B\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "zone_description_llm, zone_hidden_states_llm = [], []\n",
    "time_description_llm, time_hidden_states_llm = [], []\n",
    "day_description_llm, day_hidden_states_llm = [], []\n",
    "\n",
    "zones_points = pd.read_csv(f'{RAW_DATASET_DIR}/zone-stations.csv', usecols=[2,12,14])\n",
    "## 提取站点信息\n",
    "for zone_id, sub_points in tqdm.notebook.tqdm(zones_points.groupby('TAZID')):\n",
    "    district_id_text = sub_points['DISTRICT'].iloc[0]\n",
    "    point_id_text = '、'.join(sub_points['name'])\n",
    "    input_text = f'中国广东省深圳市{district_id_text + point_id_text}附近区域的土地利用、居民组成、交通出行模式、新能源汽车友好度、新能源汽车充电需求与服务水平。'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    zone_description_llm.append([zone_id, thinking_content, content])\n",
    "    zone_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "zone_description_llm = DataFrame(zone_description_llm, columns=['Zone Name', 'LLM Thinking', 'LLM Description'])\n",
    "zone_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/zone_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "zone_hidden_states_llm = np.array(zone_hidden_states_llm)\n",
    "## 提取时间信息\n",
    "for time in range(24):\n",
    "    # Example text to generate from\n",
    "    input_text = f'中国广东省深圳市{time:02d}:00-{time+1:02d}:00间的新能源汽车充电需求与服务水平。'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "\n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    time_description_llm.append([time, thinking_content, content])\n",
    "    time_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "time_description_llm = DataFrame(time_description_llm, columns=['Hour', 'LLM Thinking', 'LLM Description'])\n",
    "time_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/time_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "time_hidden_states_llm = np.array(time_hidden_states_llm)\n",
    "## 提取日周期信息\n",
    "for day in ['周一', '周二', '周三', '周四', '周五', '周六', '周日', '节假日']:\n",
    "    # Example text to generate from\n",
    "    input_text = f'中国广东省深圳市一个典型{day}的新能源汽车充电需求与服务水平。'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    day_description_llm.append([day, thinking_content, content])\n",
    "    day_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "day_description_llm = DataFrame(day_description_llm, columns=['Day', 'LLM Thinking', 'LLM Description'])\n",
    "day_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/day_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "day_hidden_states_llm = np.array(day_hidden_states_llm)\n",
    "    \n",
    "np.savez_compressed(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/hidden_states_{LLM_NAME_}.npz',\n",
    "                    zone=zone_hidden_states_llm, time=time_hidden_states_llm, day=day_hidden_states_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f8dd20-2781-4d32-b3f6-763dc330a623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "LLM_NAME = 'qwen3-14b'\n",
    "LLM_NAME_ = '_'.join(LLM_NAME.split('-'))\n",
    "\n",
    "# Try loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-14B\", model_max_length=4096)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-14B\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "zone_description_llm, zone_hidden_states_llm = [], []\n",
    "time_description_llm, time_hidden_states_llm = [], []\n",
    "day_description_llm, day_hidden_states_llm = [], []\n",
    "\n",
    "zones_points = pd.read_csv(f'{RAW_DATASET_DIR}/zone-stations.csv', usecols=[2,12,14])\n",
    "## 提取站点信息\n",
    "for zone_id, sub_points in tqdm.notebook.tqdm(zones_points.groupby('TAZID')):\n",
    "    district_id_text = sub_points['DISTRICT'].iloc[0]\n",
    "    point_id_text = '、'.join(sub_points['name'])\n",
    "    input_text = f'中国广东省深圳市{district_id_text + point_id_text}附近区域的土地利用、居民组成、交通出行模式、新能源汽车友好度、新能源汽车充电需求与服务水平。'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    zone_description_llm.append([zone_id, thinking_content, content])\n",
    "    zone_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "zone_description_llm = DataFrame(zone_description_llm, columns=['Zone Name', 'LLM Thinking', 'LLM Description'])\n",
    "zone_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/zone_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "zone_hidden_states_llm = np.array(zone_hidden_states_llm)\n",
    "## 提取时间信息\n",
    "for time in range(24):\n",
    "    # Example text to generate from\n",
    "    input_text = f'中国广东省深圳市{time:02d}:00-{time+1:02d}:00间的新能源汽车充电需求与服务水平。'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "\n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    time_description_llm.append([time, thinking_content, content])\n",
    "    time_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "time_description_llm = DataFrame(time_description_llm, columns=['Hour', 'LLM Thinking', 'LLM Description'])\n",
    "time_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/time_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "time_hidden_states_llm = np.array(time_hidden_states_llm)\n",
    "## 提取日周期信息\n",
    "for day in ['周一', '周二', '周三', '周四', '周五', '周六', '周日', '节假日']:\n",
    "    # Example text to generate from\n",
    "    input_text = f'中国广东省深圳市一个典型{day}的新能源汽车充电需求与服务水平。'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    day_description_llm.append([day, thinking_content, content])\n",
    "    day_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "day_description_llm = DataFrame(day_description_llm, columns=['Day', 'LLM Thinking', 'LLM Description'])\n",
    "day_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/day_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "day_hidden_states_llm = np.array(day_hidden_states_llm)\n",
    "    \n",
    "np.savez_compressed(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/hidden_states_{LLM_NAME_}.npz',\n",
    "                    zone=zone_hidden_states_llm, time=time_hidden_states_llm, day=day_hidden_states_llm)\n",
    "\n",
    "# zone_description_llm = pd.read_csv(f'{RAW_DATASET_DIR}/description-qwen3-14b/zone_description_qwen3_14b.csv.zip', usecols=[1,2,3])\n",
    "# zone_description_llm.to_excel(f'{RAW_DATASET_DIR}/description-qwen3-14b/zone_description_qwen3_14b.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "697ca351-8619-4465-ac5c-6b7cecc776a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T07:40:39.980331Z",
     "iopub.status.busy": "2025-06-09T07:40:39.979155Z",
     "iopub.status.idle": "2025-06-09T21:23:03.304161Z",
     "shell.execute_reply": "2025-06-09T21:23:03.302650Z",
     "shell.execute_reply.started": "2025-06-09T07:40:39.980276Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f6f6f9eca0446f907477a0aa725ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2219 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867d956410e54a8e880e4d2148e3dcc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d3596e5f794474ab09de570d120d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "PRIME_LLM_NAME = 'qwen3-14b'\n",
    "LLM_NAME = 'llama31-8b'\n",
    "PRIME_LLM_NAME_ = '_'.join(PRIME_LLM_NAME.split('-'))\n",
    "LLM_NAME_ = PRIME_LLM_NAME_ + '_' + '_'.join(LLM_NAME.split('-'))\n",
    "\n",
    "# Try loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/xcc/peng_c/llama/Llama3.1-8B-Instruct\", model_max_length=2560)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/home/xcc/peng_c/llama/Llama3.1-8B-Instruct-hg\", device_map=\"auto\")\n",
    "# # Convert the model to half precision (FP16) if using a supported device\n",
    "# model.half()  # This converts the model to FP16\n",
    "\n",
    "zone_description_llm, zone_hidden_states_llm = [], []\n",
    "time_description_llm, time_hidden_states_llm = [], []\n",
    "day_description_llm, day_hidden_states_llm = [], []\n",
    "\n",
    "zone_info = pd.read_csv(f'{RAW_DATASET_DIR}/description-{PRIME_LLM_NAME}/zone_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "time_info = pd.read_csv(f'{RAW_DATASET_DIR}/description-{PRIME_LLM_NAME}/time_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "day_info = pd.read_csv(f'{RAW_DATASET_DIR}/description-{PRIME_LLM_NAME}/day_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "## 提取站点信息\n",
    "for zone_id, prime_llm_output in tqdm.notebook.tqdm(zip(zone_info['Zone Name'], zone_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=5120, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "    \n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    zone_description_llm.append([zone_id, generated_text])\n",
    "    zone_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "zone_description_llm = DataFrame(zone_description_llm, columns=['Zone Name', 'LLM Description'])\n",
    "zone_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/zone_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "zone_hidden_states_llm = np.array(zone_hidden_states_llm)\n",
    "## 提取时间信息\n",
    "for time, prime_llm_output in tqdm.notebook.tqdm(zip(time_info['Hour'], time_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=3072, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "\n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    time_description_llm.append([time, generated_text])\n",
    "    time_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "time_description_llm = DataFrame(time_description_llm, columns=['Hour', 'LLM Description'])\n",
    "time_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/time_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "time_hidden_states_llm = np.array(time_hidden_states_llm)\n",
    "## 提取日周期信息\n",
    "for day, prime_llm_output in tqdm.notebook.tqdm(zip(day_info['Day'], day_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=3072, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "    \n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    day_description_llm.append([day, generated_text])\n",
    "    day_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "day_description_llm = DataFrame(day_description_llm, columns=['Day', 'LLM Description'])\n",
    "day_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/day_description_{LLM_NAME_}csv.zip', compression='zip')\n",
    "day_hidden_states_llm = np.array(day_hidden_states_llm)\n",
    "    \n",
    "np.savez_compressed(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/hidden_states_{LLM_NAME_}.npz',\n",
    "                    zone=zone_hidden_states_llm, time=time_hidden_states_llm, day=day_hidden_states_llm)\n",
    "# zone_description_llm = pd.read_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/zone_description_{LLM_NAME_}.csv.zip', usecols=[1,2])\n",
    "# zone_description_llm.to_excel(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/zone_description_{LLM_NAME_}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136d9629-ff9a-4043-ad92-390987aee898",
   "metadata": {},
   "source": [
    "# METR-LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6885747a-9adf-48e0-ba2d-929614ec9dd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T05:58:06.383541Z",
     "iopub.status.busy": "2025-07-05T05:58:06.383541Z",
     "iopub.status.idle": "2025-07-05T05:58:06.397542Z",
     "shell.execute_reply": "2025-07-05T05:58:06.397542Z",
     "shell.execute_reply.started": "2025-07-05T05:58:06.383541Z"
    }
   },
   "outputs": [],
   "source": [
    "RAW_DATASET_DIR = 'metr-la'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "234d2a46-6dfe-43c0-a9f3-b8be13b94fcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T21:30:05.749403Z",
     "iopub.status.busy": "2025-06-09T21:30:05.748808Z",
     "iopub.status.idle": "2025-06-09T23:25:31.057103Z",
     "shell.execute_reply": "2025-06-09T23:25:31.055225Z",
     "shell.execute_reply.started": "2025-06-09T21:30:05.749354Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-10 05:30:07,356] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xcc/miniconda3/envs/peng_c/compiler_compat/ld: cannot find -laio: 没有那个文件或目录\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/xcc/miniconda3/envs/peng_c/compiler_compat/ld: cannot find -lcufile: 没有那个文件或目录\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faea2f7d9dd6409e906a97fb40f88718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae200de551814d2aabe011045b26ff7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d9a783d3084f708ddc9e15086baa80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "LLM_NAME = 'llama31-8b'\n",
    "LLM_NAME_ = '_'.join(LLM_NAME.split('-'))\n",
    "\n",
    "# Try loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Llama3.1-8B-Instruct\", model_max_length=1024)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Llama3.1-8B-Instruct-hg\", device_map=\"auto\")\n",
    "# # Convert the model to half precision (FP16) if using a supported device\n",
    "# model.half()  # This converts the model to FP16\n",
    "\n",
    "station_description_llm, station_hidden_states_llm = [], []\n",
    "time_description_llm, time_hidden_states_llm = [], []\n",
    "day_description_llm, day_hidden_states_llm = [], []\n",
    "\n",
    "station_info = pd.read_csv(f'{RAW_DATASET_DIR}/graph_sensor_metadata.csv', usecols=[1, 5, 12, 13, 14, 15])\n",
    "## 提取站点信息\n",
    "for _, det_id, fwy_dir, det_name, det_county, fwy_name, det_city in tqdm.notebook.tqdm(station_info.itertuples()):\n",
    "    fwy_name = fwy_name + '-' + fwy_dir\n",
    "    # Example text to generate from\n",
    "    input_text = f'Please provide details about the location, traffic demand pattern, and traffic condition on a segment of the {fwy_name} Freeway around {det_name} ' +\\\n",
    "                 f'in {det_city}, {det_county} County, California, USA.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=4096, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "    \n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'])\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    station_description_llm.append([det_id, generated_text])\n",
    "    station_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "station_description_llm = DataFrame(station_description_llm, columns=['Sensor Name', 'LLM Description'])\n",
    "station_description_llm.to_csv(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/station_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "station_hidden_states_llm = np.array(station_hidden_states_llm)\n",
    "## 提取时间信息\n",
    "fwy_names = ', '.join(set(station_info['FwyName']))\n",
    "city_names = 'Burbank, Glendale, La Canada-Flintridge, and Los Angeles'\n",
    "for time in tqdm.notebook.tqdm(pd.date_range(start='2000-01-01 00:00', end='2000-01-01 23:55', freq='15min').strftime('%H:%M')):\n",
    "    # Example text to generate from\n",
    "    input_text = f'Typical traffic demands and conditions at {time} on {fwy_names} in {city_names}, Los Angeles County, California, USA.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=2048, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "\n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'])\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    time_description_llm.append([time, generated_text])\n",
    "    time_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "time_description_llm = DataFrame(time_description_llm, columns=['Time', 'LLM Description'])\n",
    "time_description_llm.to_csv(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/time_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "time_hidden_states_llm = np.array(time_hidden_states_llm)\n",
    "## 提取日周期信息\n",
    "for day in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']:\n",
    "    # Example text to generate from\n",
    "    input_text = f'Traffic demands and conditions on a typical {day} on {fwy_names} in {city_names}, Los Angeles County, California, USA.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=2048, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "    \n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'])\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    day_description_llm.append([day, generated_text])\n",
    "    day_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "day_description_llm = DataFrame(day_description_llm, columns=['Day', 'LLM Description'])\n",
    "day_description_llm.to_csv(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/day_description_{LLM_NAME_}csv.zip', compression='zip')\n",
    "day_hidden_states_llm = np.array(day_hidden_states_llm)\n",
    "    \n",
    "np.savez_compressed(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/hidden_states_{LLM_NAME_}.npz',\n",
    "                    station=station_hidden_states_llm, time=time_hidden_states_llm, day=day_hidden_states_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bd1c1a-8a8a-4445-acd9-c958a465b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "LLM_NAME = 'qwen3-8b'\n",
    "LLM_NAME_ = '_'.join(LLM_NAME.split('-'))\n",
    "\n",
    "# Try loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", model_max_length=1024)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-8B\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "station_description_llm, station_hidden_states_llm = [], []\n",
    "time_description_llm, time_hidden_states_llm = [], []\n",
    "day_description_llm, day_hidden_states_llm = [], []\n",
    "\n",
    "station_info = pd.read_csv(f'{RAW_DATASET_DIR}/graph_sensor_metadata.csv', usecols=[1, 5, 12, 13, 14, 15])\n",
    "## 提取站点信息\n",
    "for _, det_id, fwy_dir, det_name, det_county, fwy_name, det_city in tqdm.notebook.tqdm(station_info.itertuples()):\n",
    "    fwy_name = fwy_name + '-' + fwy_dir\n",
    "    # Example text to generate from\n",
    "    input_text = f'Please provide details about the location, traffic demand pattern, and traffic condition on a segment of the {fwy_name} Freeway around {det_name} ' +\\\n",
    "                 f'in {det_city}, {det_county} County, California, USA.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    station_description_llm.append([det_id, thinking_content, content])\n",
    "    station_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "station_description_llm = DataFrame(station_description_llm, columns=['Sensor Name', 'LLM Thinking', 'LLM Description'])\n",
    "station_description_llm.to_csv(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/station_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "station_hidden_states_llm = np.array(station_hidden_states_llm)\n",
    "## 提取时间信息\n",
    "fwy_names = ', '.join(set(station_info['FwyName']))\n",
    "city_names = 'Burbank, Glendale, La Canada-Flintridge, and Los Angeles'\n",
    "for time in tqdm.notebook.tqdm(pd.date_range(start='2000-01-01 00:00', end='2000-01-01 23:55', freq='15min').strftime('%H:%M')):\n",
    "    # Example text to generate from\n",
    "    input_text = f'Typical traffic demands and conditions at {time} on {fwy_names} in {city_names}, Los Angeles County, California, USA.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "\n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    time_description_llm.append([time, thinking_content, content])\n",
    "    time_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "time_description_llm = DataFrame(time_description_llm, columns=['Time', 'LLM Thinking', 'LLM Description'])\n",
    "time_description_llm.to_csv(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/time_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "time_hidden_states_llm = np.array(time_hidden_states_llm)\n",
    "## 提取日周期信息\n",
    "for day in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']:\n",
    "    # Example text to generate from\n",
    "    input_text = f'Traffic demands and conditions on a typical {day} on {fwy_names} in {city_names}, Los Angeles County, California, USA.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    day_description_llm.append([day, thinking_content, content])\n",
    "    day_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "day_description_llm = DataFrame(day_description_llm, columns=['Day', 'LLM Thinking', 'LLM Description'])\n",
    "day_description_llm.to_csv(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/day_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "day_hidden_states_llm = np.array(day_hidden_states_llm)\n",
    "    \n",
    "np.savez_compressed(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/hidden_states_{LLM_NAME_}.npz',\n",
    "                    station=station_hidden_states_llm, time=time_hidden_states_llm, day=day_hidden_states_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ccb166-410f-4be3-b702-d492828f5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "LLM_NAME = 'qwen3-14b'\n",
    "LLM_NAME_ = '_'.join(LLM_NAME.split('-'))\n",
    "\n",
    "# Try loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-14B\", model_max_length=1024)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-14B\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "station_description_llm, station_hidden_states_llm = [], []\n",
    "time_description_llm, time_hidden_states_llm = [], []\n",
    "day_description_llm, day_hidden_states_llm = [], []\n",
    "\n",
    "station_info = pd.read_csv(f'{RAW_DATASET_DIR}/graph_sensor_metadata.csv', usecols=[1, 5, 12, 13, 14, 15])\n",
    "## 提取站点信息\n",
    "for _, det_id, fwy_dir, det_name, det_county, fwy_name, det_city in tqdm.notebook.tqdm(station_info.itertuples()):\n",
    "    fwy_name = fwy_name + '-' + fwy_dir\n",
    "    # Example text to generate from\n",
    "    input_text = f'Please provide details about the location, traffic demand pattern, and traffic condition on a segment of the {fwy_name} Freeway around {det_name} ' +\\\n",
    "                 f'in {det_city}, {det_county} County, California, USA.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    station_description_llm.append([det_id, thinking_content, content])\n",
    "    station_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "station_description_llm = DataFrame(station_description_llm, columns=['Sensor Name', 'LLM Thinking', 'LLM Description'])\n",
    "station_description_llm.to_csv(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/station_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "station_hidden_states_llm = np.array(station_hidden_states_llm)\n",
    "## 提取时间信息\n",
    "fwy_names = ', '.join(set(station_info['FwyName']))\n",
    "city_names = 'Burbank, Glendale, La Canada-Flintridge, and Los Angeles'\n",
    "for time in tqdm.notebook.tqdm(pd.date_range(start='2000-01-01 00:00', end='2000-01-01 23:55', freq='15min').strftime('%H:%M')):\n",
    "    # Example text to generate from\n",
    "    input_text = f'Typical traffic demands and conditions at {time} on {fwy_names} in {city_names}, Los Angeles County, California, USA.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "\n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    time_description_llm.append([time, thinking_content, content])\n",
    "    time_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "time_description_llm = DataFrame(time_description_llm, columns=['Time', 'LLM Thinking', 'LLM Description'])\n",
    "time_description_llm.to_csv(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/time_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "time_hidden_states_llm = np.array(time_hidden_states_llm)\n",
    "## 提取日周期信息\n",
    "for day in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']:\n",
    "    # Example text to generate from\n",
    "    input_text = f'Traffic demands and conditions on a typical {day} on {fwy_names} in {city_names}, Los Angeles County, California, USA.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    day_description_llm.append([day, thinking_content, content])\n",
    "    day_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "day_description_llm = DataFrame(day_description_llm, columns=['Day', 'LLM Thinking', 'LLM Description'])\n",
    "day_description_llm.to_csv(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/day_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "day_hidden_states_llm = np.array(day_hidden_states_llm)\n",
    "    \n",
    "np.savez_compressed(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/hidden_states_{LLM_NAME_}.npz',\n",
    "                    station=station_hidden_states_llm, time=time_hidden_states_llm, day=day_hidden_states_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ec1c810-c3ae-40b3-b39b-e18bcb7f40dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T11:50:12.454668Z",
     "iopub.status.busy": "2025-06-05T11:50:12.454475Z",
     "iopub.status.idle": "2025-06-05T17:38:15.498401Z",
     "shell.execute_reply": "2025-06-05T17:38:15.496737Z",
     "shell.execute_reply.started": "2025-06-05T11:50:12.454653Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959233b6d31c4213bde72ba06251c3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1097 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9e71c7491e4f448b60df0b25c1186e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9cfdb1928446c1ac72bfe61c94be0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "PRIME_LLM_NAME = 'qwen3-14b'\n",
    "LLM_NAME = 'llama31-8b'\n",
    "PRIME_LLM_NAME_ = '_'.join(PRIME_LLM_NAME.split('-'))\n",
    "LLM_NAME_ = PRIME_LLM_NAME_ + '_' + '_'.join(LLM_NAME.split('-'))\n",
    "\n",
    "# Try loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Llama3.1-8B-Instruct\", model_max_length=2048)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Llama3.1-8B-Instruct-hg\", device_map=\"auto\")\n",
    "# # Convert the model to half precision (FP16) if using a supported device\n",
    "# model.half()  # This converts the model to FP16\n",
    "\n",
    "station_description_llm, station_hidden_states_llm = [], []\n",
    "time_description_llm, time_hidden_states_llm = [], []\n",
    "day_description_llm, day_hidden_states_llm = [], []\n",
    "\n",
    "station_info = pd.read_csv(f'{RAW_DATASET_DIR}/llm-description/description-{PRIME_LLM_NAME}/station_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "time_info = pd.read_csv(f'{RAW_DATASET_DIR}/llm-description/description-{PRIME_LLM_NAME}/time_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "day_info = pd.read_csv(f'{RAW_DATASET_DIR}/llm-description/description-{PRIME_LLM_NAME}/day_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "## 提取站点信息\n",
    "for station_id, prime_llm_output in tqdm.notebook.tqdm(zip(station_info['Sensor Name'], station_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=4096, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "    \n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    station_description_llm.append([station_id, generated_text])\n",
    "    station_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "station_description_llm = DataFrame(station_description_llm, columns=['Sensor Name', 'LLM Description'])\n",
    "station_description_llm.to_csv(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/station_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "station_hidden_states_llm = np.array(station_hidden_states_llm)\n",
    "## 提取时间信息\n",
    "for time, prime_llm_output in tqdm.notebook.tqdm(zip(time_info['Time'], time_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=3072, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "\n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    time_description_llm.append([time, generated_text])\n",
    "    time_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "time_description_llm = DataFrame(time_description_llm, columns=['Time', 'LLM Description'])\n",
    "time_description_llm.to_csv(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/time_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "time_hidden_states_llm = np.array(time_hidden_states_llm)\n",
    "## 提取日周期信息\n",
    "for day, prime_llm_output in tqdm.notebook.tqdm(zip(day_info['Day'], day_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=3072, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "    \n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    day_description_llm.append([day, generated_text])\n",
    "    day_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "day_description_llm = DataFrame(day_description_llm, columns=['Day', 'LLM Description'])\n",
    "day_description_llm.to_csv(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/day_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "day_hidden_states_llm = np.array(day_hidden_states_llm)\n",
    "    \n",
    "np.savez_compressed(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/hidden_states_{LLM_NAME_}.npz',\n",
    "                    station=station_hidden_states_llm, time=time_hidden_states_llm, day=day_hidden_states_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfde505-14cb-42cb-acf2-ddc56402b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "PRIME_LLM_NAME = 'qwen3-14b'\n",
    "LLM_NAME = 'qwen3-8b'\n",
    "PRIME_LLM_NAME_ = '_'.join(PRIME_LLM_NAME.split('-'))\n",
    "LLM_NAME_ = PRIME_LLM_NAME_ + '_' + '_'.join(LLM_NAME.split('-'))\n",
    "\n",
    "# Try loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", model_max_length=2048)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-8B\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "station_description_llm, station_hidden_states_llm = [], []\n",
    "time_description_llm, time_hidden_states_llm = [], []\n",
    "day_description_llm, day_hidden_states_llm = [], []\n",
    "\n",
    "station_info = pd.read_csv(f'{RAW_DATASET_DIR}/llm-description/description-{PRIME_LLM_NAME}/station_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "time_info = pd.read_csv(f'{RAW_DATASET_DIR}/llm-description/description-{PRIME_LLM_NAME}/time_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "day_info = pd.read_csv(f'{RAW_DATASET_DIR}/llm-description/description-{PRIME_LLM_NAME}/day_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "## 提取站点信息\n",
    "for station_id, prime_llm_output in tqdm.notebook.tqdm(zip(station_info['Sensor Name'], station_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    station_description_llm.append([station_id, thinking_content, content])\n",
    "    station_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "station_description_llm = DataFrame(station_description_llm, columns=['Sensor Name', 'LLM Thinking', 'LLM Description'])\n",
    "station_description_llm.to_csv(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/station_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "station_hidden_states_llm = np.array(station_hidden_states_llm)\n",
    "## 提取时间信息\n",
    "for time, prime_llm_output in tqdm.notebook.tqdm(zip(time_info['Time'], time_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "\n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    time_description_llm.append([time, thinking_content, content])\n",
    "    time_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "time_description_llm = DataFrame(time_description_llm, columns=['Time', 'LLM Thinking', 'LLM Description'])\n",
    "time_description_llm.to_csv(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/time_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "time_hidden_states_llm = np.array(time_hidden_states_llm)\n",
    "## 提取日周期信息\n",
    "for day, prime_llm_output in tqdm.notebook.tqdm(zip(day_info['Day'], day_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    day_description_llm.append([day, thinking_content, content])\n",
    "    day_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "day_description_llm = DataFrame(day_description_llm, columns=['Day', 'LLM Thinking', 'LLM Description'])\n",
    "day_description_llm.to_csv(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/day_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "day_hidden_states_llm = np.array(day_hidden_states_llm)\n",
    "    \n",
    "np.savez_compressed(f'{RAW_DATASET_DIR}/llm-description/description-{LLM_NAME}/hidden_states_{LLM_NAME_}.npz',\n",
    "                    station=station_hidden_states_llm, time=time_hidden_states_llm, day=day_hidden_states_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971e172-a865-4af2-9204-5bf463351efe",
   "metadata": {},
   "source": [
    "# subway-mta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cf466eb-1c06-4559-b760-dc5d2ed4cd7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T06:00:40.836182Z",
     "iopub.status.busy": "2025-07-06T06:00:40.835183Z",
     "iopub.status.idle": "2025-07-06T06:00:40.854182Z",
     "shell.execute_reply": "2025-07-06T06:00:40.854182Z",
     "shell.execute_reply.started": "2025-07-06T06:00:40.836182Z"
    }
   },
   "outputs": [],
   "source": [
    "RAW_DATASET_DIR = 'subway-mta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd59b3b-e9f5-4c6e-a8aa-b81986f06f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "LLM_NAME = 'llama31-8b'\n",
    "LLM_NAME_ = '_'.join(LLM_NAME.split('-'))\n",
    "\n",
    "# Try loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Llama3.1-8B-Instruct\", model_max_length=1024)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Llama3.1-8B-Instruct-hg\", device_map=\"auto\")\n",
    "# # Convert the model to half precision (FP16) if using a supported device\n",
    "# model.half()  # This converts the model to FP16\n",
    "\n",
    "station_description_llm, station_hidden_states_llm = [], []\n",
    "time_description_llm, time_hidden_states_llm = [], []\n",
    "day_description_llm, day_hidden_states_llm = [], []\n",
    "\n",
    "station_info = pd.read_csv(f'{RAW_DATASET_DIR}/MTA_Subway_Hourly_Ridership__2020-2024_20250317.zip', usecols=[3, 4])\n",
    "station_info.drop_duplicates(inplace=True)\n",
    "station_info.set_index('station_complex', inplace=True)\n",
    "## 提取站点信息\n",
    "for station_id, borough_id in tqdm.notebook.tqdm(zip(station_info.index, station_info['borough'])):\n",
    "    # Example text to generate from\n",
    "    input_text = f'Please provide details about the location, train operations, ridership demand, and landmarks nearby about the {station_id} Station ' +\\\n",
    "                 f'in {borough_id} of the New York City Subway System.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=4096, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "    \n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'])\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    station_description_llm.append([station_id, generated_text])\n",
    "    station_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "station_description_llm = DataFrame(station_description_llm, columns=['Station Name', 'LLM Description'])\n",
    "station_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/station_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "station_hidden_states_llm = np.array(station_hidden_states_llm)\n",
    "## 提取时间信息\n",
    "for time in range(24):\n",
    "    # Example text to generate from\n",
    "    input_text = f'New York City Subway System during {time:02d}:00-{time+1:02d}:00, service and ridership.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=4096, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "\n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'])\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    time_description_llm.append([time, generated_text])\n",
    "    time_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "time_description_llm = DataFrame(time_description_llm, columns=['Hour', 'LLM Description'])\n",
    "time_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/time_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "time_hidden_states_llm = np.array(time_hidden_states_llm)\n",
    "## 提取日周期信息\n",
    "for day in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']:\n",
    "    # Example text to generate from\n",
    "    input_text = f'New York City Subway System  on a typical {day}, service and ridership.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=4096, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "    \n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'])\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    day_description_llm.append([day, generated_text])\n",
    "    day_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "day_description_llm = DataFrame(day_description_llm, columns=['Day', 'LLM Description'])\n",
    "day_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/day_description_{LLM_NAME_}csv.zip', compression='zip')\n",
    "day_hidden_states_llm = np.array(day_hidden_states_llm)\n",
    "    \n",
    "np.savez_compressed(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/hidden_states_{LLM_NAME_}.npz', station=station_hidden_states_llm, time=time_hidden_states_llm, day=day_hidden_states_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e872c-a473-42ba-b0bd-89556a9447d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "LLM_NAME = 'qwen3-8b'\n",
    "LLM_NAME_ = '_'.join(LLM_NAME.split('-'))\n",
    "\n",
    "# Try loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", model_max_length=1024)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-8B\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "station_description_llm, station_hidden_states_llm = [], []\n",
    "time_description_llm, time_hidden_states_llm = [], []\n",
    "day_description_llm, day_hidden_states_llm = [], []\n",
    "\n",
    "station_info = pd.read_csv(f'{RAW_DATASET_DIR}/MTA_Subway_Hourly_Ridership__2020-2024_20250317.zip', usecols=[3, 4])\n",
    "station_info.drop_duplicates(inplace=True)\n",
    "station_info.set_index('station_complex', inplace=True)\n",
    "## 提取站点信息\n",
    "for station_id, borough_id in tqdm.notebook.tqdm(zip(station_info.index, station_info['borough'])):\n",
    "    # Example text to generate from\n",
    "    input_text = f'Please provide details about the location, train operations, ridership demand, and landmarks nearby about the {station_id} Station ' +\\\n",
    "                 f'in {borough_id} of the New York City Subway System.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=16384, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    station_description_llm.append([station_id, thinking_content, content])\n",
    "    station_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "station_description_llm = DataFrame(station_description_llm, columns=['Station Name', 'LLM Thinking', 'LLM Description'])\n",
    "station_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/station_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "station_hidden_states_llm = np.array(station_hidden_states_llm)\n",
    "## 提取时间信息\n",
    "for time in range(24):\n",
    "    # Example text to generate from\n",
    "    input_text = f'New York City Subway System during {time:02d}:00-{time+1:02d}:00, service and ridership.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "\n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    time_description_llm.append([time, thinking_content, content])\n",
    "    time_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "time_description_llm = DataFrame(time_description_llm, columns=['Hour', 'LLM Thinking', 'LLM Description'])\n",
    "time_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/time_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "time_hidden_states_llm = np.array(time_hidden_states_llm)\n",
    "## 提取日周期信息\n",
    "for day in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']:\n",
    "    # Example text to generate from\n",
    "    input_text = f'New York City Subway System  on a typical {day}, service and ridership.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    day_description_llm.append([day, thinking_content, content])\n",
    "    day_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "day_description_llm = DataFrame(day_description_llm, columns=['Day', 'LLM Thinking', 'LLM Description'])\n",
    "day_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/day_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "day_hidden_states_llm = np.array(day_hidden_states_llm)\n",
    "    \n",
    "np.savez_compressed(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/hidden_states_{LLM_NAME_}.npz', station=station_hidden_states_llm, time=time_hidden_states_llm, day=day_hidden_states_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c668ed-fea6-42e4-b577-23135b59ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "LLM_NAME = 'qwen3-14b'\n",
    "LLM_NAME_ = '_'.join(LLM_NAME.split('-'))\n",
    "\n",
    "# Try loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-14B\", model_max_length=1024)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-14B\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "station_description_llm, station_hidden_states_llm = [], []\n",
    "time_description_llm, time_hidden_states_llm = [], []\n",
    "day_description_llm, day_hidden_states_llm = [], []\n",
    "\n",
    "station_info = pd.read_csv(f'{RAW_DATASET_DIR}/MTA_Subway_Hourly_Ridership__2020-2024_20250317.zip', usecols=[3, 4])\n",
    "station_info.drop_duplicates(inplace=True)\n",
    "station_info.set_index('station_complex', inplace=True)\n",
    "## 提取站点信息\n",
    "for station_id, borough_id in tqdm.notebook.tqdm(zip(station_info.index, station_info['borough'])):\n",
    "    # Example text to generate from\n",
    "    input_text = f'Please provide details about the location, train operations, ridership demand, and landmarks nearby about the {station_id} Station ' +\\\n",
    "                 f'in {borough_id} of the New York City Subway System.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    station_description_llm.append([station_id, thinking_content, content])\n",
    "    station_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "station_description_llm = DataFrame(station_description_llm, columns=['Station Name', 'LLM Thinking', 'LLM Description'])\n",
    "station_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/station_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "station_hidden_states_llm = np.array(station_hidden_states_llm)\n",
    "## 提取时间信息\n",
    "for time in range(24):\n",
    "    # Example text to generate from\n",
    "    input_text = f'New York City Subway System during {time:02d}:00-{time+1:02d}:00, service and ridership.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "\n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    time_description_llm.append([time, thinking_content, content])\n",
    "    time_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "time_description_llm = DataFrame(time_description_llm, columns=['Hour', 'LLM Thinking', 'LLM Description'])\n",
    "time_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/time_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "time_hidden_states_llm = np.array(time_hidden_states_llm)\n",
    "## 提取日周期信息\n",
    "for day in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']:\n",
    "    # Example text to generate from\n",
    "    input_text = f'New York City Subway System  on a typical {day}, service and ridership.'\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    day_description_llm.append([day, thinking_content, content])\n",
    "    day_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "day_description_llm = DataFrame(day_description_llm, columns=['Day', 'LLM Thinking', 'LLM Description'])\n",
    "day_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/day_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "day_hidden_states_llm = np.array(day_hidden_states_llm)\n",
    "    \n",
    "np.savez_compressed(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/hidden_states_{LLM_NAME_}.npz', station=station_hidden_states_llm, time=time_hidden_states_llm, day=day_hidden_states_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65038480-1be5-4100-a442-ddde30484145",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T13:03:04.571589Z",
     "iopub.status.busy": "2025-06-03T13:03:04.570993Z",
     "iopub.status.idle": "2025-06-04T01:02:55.585975Z",
     "shell.execute_reply": "2025-06-04T01:02:55.584488Z",
     "shell.execute_reply.started": "2025-06-03T13:03:04.571540Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562e84fefa084bc88926b444a39dd91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5378770c334f4095a4e04f90008ff0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d29949f4aa486fa9667e736b7219de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "PRIME_LLM_NAME = 'qwen3-14b'\n",
    "LLM_NAME = 'llama31-8b'\n",
    "PRIME_LLM_NAME_ = '_'.join(PRIME_LLM_NAME.split('-'))\n",
    "LLM_NAME_ = PRIME_LLM_NAME_ + '_' + '_'.join(LLM_NAME.split('-'))\n",
    "\n",
    "# Try loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Llama3.1-8B-Instruct\", model_max_length=2048)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Llama3.1-8B-Instruct-hg\", device_map=\"auto\")\n",
    "# # Convert the model to half precision (FP16) if using a supported device\n",
    "# model.half()  # This converts the model to FP16\n",
    "\n",
    "station_description_llm, station_hidden_states_llm = [], []\n",
    "time_description_llm, time_hidden_states_llm = [], []\n",
    "day_description_llm, day_hidden_states_llm = [], []\n",
    "\n",
    "station_info = pd.read_csv(f'{RAW_DATASET_DIR}/description-{PRIME_LLM_NAME}/station_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "time_info = pd.read_csv(f'{RAW_DATASET_DIR}/description-{PRIME_LLM_NAME}/time_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "day_info = pd.read_csv(f'{RAW_DATASET_DIR}/description-{PRIME_LLM_NAME}/day_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "## 提取站点信息\n",
    "for station_id, prime_llm_output in tqdm.notebook.tqdm(zip(station_info['Station Name'], station_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=4096, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "    \n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    station_description_llm.append([station_id, generated_text])\n",
    "    station_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "station_description_llm = DataFrame(station_description_llm, columns=['Station Name', 'LLM Description'])\n",
    "station_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/station_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "station_hidden_states_llm = np.array(station_hidden_states_llm)\n",
    "## 提取时间信息\n",
    "for time, prime_llm_output in tqdm.notebook.tqdm(zip(time_info['Hour'], time_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=4096, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "\n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    time_description_llm.append([time, generated_text])\n",
    "    time_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "time_description_llm = DataFrame(time_description_llm, columns=['Hour', 'LLM Description'])\n",
    "time_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/time_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "time_hidden_states_llm = np.array(time_hidden_states_llm)\n",
    "## 提取日周期信息\n",
    "for day, prime_llm_output in tqdm.notebook.tqdm(zip(day_info['Day'], day_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=4096, return_dict_in_generate=True,\n",
    "                                 output_hidden_states=True)\n",
    "    \n",
    "    generated_text = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    day_description_llm.append([day, generated_text])\n",
    "    day_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "day_description_llm = DataFrame(day_description_llm, columns=['Day', 'LLM Description'])\n",
    "day_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/day_description_{LLM_NAME_}csv.zip', compression='zip')\n",
    "day_hidden_states_llm = np.array(day_hidden_states_llm)\n",
    "    \n",
    "np.savez_compressed(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/hidden_states_{LLM_NAME_}.npz', station=station_hidden_states_llm, time=time_hidden_states_llm, day=day_hidden_states_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab4eadb-99bb-4697-930c-84a5aefd257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "PRIME_LLM_NAME = 'qwen3-14b'\n",
    "LLM_NAME = 'qwen3-8b'\n",
    "PRIME_LLM_NAME_ = '_'.join(PRIME_LLM_NAME.split('-'))\n",
    "LLM_NAME_ = PRIME_LLM_NAME_ + '_' + '_'.join(LLM_NAME.split('-'))\n",
    "\n",
    "# Try loading the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", model_max_length=2048)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-8B\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "station_description_llm, station_hidden_states_llm = [], []\n",
    "time_description_llm, time_hidden_states_llm = [], []\n",
    "day_description_llm, day_hidden_states_llm = [], []\n",
    "\n",
    "station_info = pd.read_csv(f'{RAW_DATASET_DIR}/description-{PRIME_LLM_NAME}/station_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "time_info = pd.read_csv(f'{RAW_DATASET_DIR}/description-{PRIME_LLM_NAME}/time_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "day_info = pd.read_csv(f'{RAW_DATASET_DIR}/description-{PRIME_LLM_NAME}/day_description_{PRIME_LLM_NAME_}.csv.zip', index_col=0)\n",
    "## 提取站点信息\n",
    "for station_id, prime_llm_output in tqdm.notebook.tqdm(zip(station_info['Station Name'], station_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=16384, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    station_description_llm.append([station_id, thinking_content, content])\n",
    "    station_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "station_description_llm = DataFrame(station_description_llm, columns=['Station Name', 'LLM Thinking', 'LLM Description'])\n",
    "station_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/station_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "station_hidden_states_llm = np.array(station_hidden_states_llm)\n",
    "## 提取时间信息\n",
    "for time, prime_llm_output in tqdm.notebook.tqdm(zip(time_info['Hour'], time_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "\n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    time_description_llm.append([time, thinking_content, content])\n",
    "    time_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "time_description_llm = DataFrame(time_description_llm, columns=['Hour', 'LLM Thinking', 'LLM Description'])\n",
    "time_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/time_description_{LLM_NAME_}.csv.zip', compression='zip')\n",
    "time_hidden_states_llm = np.array(time_hidden_states_llm)\n",
    "## 提取日周期信息\n",
    "for day, prime_llm_output in tqdm.notebook.tqdm(zip(day_info['Day'], day_info['LLM Description'])):\n",
    "    # Example text to generate from\n",
    "    input_text = 'Paraphrase the following contend.\\n' + prime_llm_output \n",
    "    # Encode input text\n",
    "    inputs = tokenize_llm_inputs(tokenizer, llm_name=LLM_NAME.split('-')[0], input_text=input_text)\n",
    "    # conduct text completion\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_new_tokens=32768, return_dict_in_generate=True, output_hidden_states=True)\n",
    "    \n",
    "    thinking_content, content = decode_llm_outputs(tokenizer, llm_name=LLM_NAME.split('-')[0], output_sequences=outputs['sequences'], input_len=len(inputs['input_ids'][0]))\n",
    "    hidden_states = torch.vstack(outputs.hidden_states[-1]).squeeze() # shape: [41, 5120]; hidden_states[-1] 表示最后一个 token，41 表示 Qwen3-14b 的 41 层隐状态，5120 表示每个隐向量的维数\n",
    "    day_description_llm.append([day, thinking_content, content])\n",
    "    day_hidden_states_llm.append(hidden_states.cpu().float().numpy())\n",
    "    del inputs\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache() # 释放内存\n",
    "day_description_llm = DataFrame(day_description_llm, columns=['Day', 'LLM Thinking', 'LLM Description'])\n",
    "day_description_llm.to_csv(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/day_description_{LLM_NAME_}csv.zip', compression='zip')\n",
    "day_hidden_states_llm = np.array(day_hidden_states_llm)\n",
    "    \n",
    "np.savez_compressed(f'{RAW_DATASET_DIR}/description-{LLM_NAME}/hidden_states_{LLM_NAME_}.npz',\n",
    "                    station=station_hidden_states_llm, time=time_hidden_states_llm, day=day_hidden_states_llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
